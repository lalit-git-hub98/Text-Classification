{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZjoE8gES9ErcrKsW4LdXO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Text Classification\n","\n","Text classification is the process of assigning predefined categories or labels to text data. It's a fundamental task in natural language processing (NLP) and machine learning, with applications ranging from spam detection and sentiment analysis to topic categorization and language identification. Here's an overview of text classification and its different approaches:\n","\n","## Approaches to Text Classification\n","\n","### Rule-Based Systems\n","\n","- **Description:** These systems use predefined rules created by experts to classify text.\n","- **Advantages:** Simple to implement and understand.\n","- **Disadvantages:** Rules can be rigid and may not generalize well to new, unseen data.\n","\n","### Machine Learning Approaches\n","\n","#### Traditional Machine Learning\n","\n","- **Algorithms:** Naive Bayes, Support Vector Machines (SVM), Logistic Regression, Decision Trees, and Random Forests.\n","- **Process:** Involves feature extraction (e.g., TF-IDF, word embeddings) followed by training a model on labeled data.\n","- **Advantages:** Can handle large datasets and capture complex patterns.\n","- **Disadvantages:** Requires feature engineering and may not capture semantic meaning well.\n","\n","#### Deep Learning\n","\n","- **Algorithms:** Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Transformers (e.g., BERT).\n","- **Process:** Involves training neural networks on raw text data, often using word embeddings or contextual embeddings.\n","- **Advantages:** Can capture complex, non-linear relationships and semantic meaning.\n","- **Disadvantages:** Requires large amounts of data and computational resources.\n","\n","### Hybrid Approaches\n","\n","- **Description:** Combines rule-based systems with machine learning models to leverage the strengths of both.\n","- **Advantages:** Can improve accuracy and robustness.\n","- **Disadvantages:** More complex to implement and maintain.\n","\n","### Transfer Learning\n","\n","- **Description:** Uses pre-trained models (e.g., BERT, RoBERTa) fine-tuned on specific text classification tasks.\n","- **Advantages:** Can achieve high accuracy with less data.\n","- **Disadvantages:** May require fine-tuning and computational resources.\n","\n","## Key Considerations\n","\n","- **Data Quality:** The quality and quantity of labeled data significantly impact model performance.\n","- **Feature Engineering:** Traditional machine learning models require careful feature selection and extraction.\n","- **Model Selection:** The choice of model depends on the specific task, dataset size, and computational resources.\n","- **Evaluation Metrics:** Common metrics include accuracy, precision, recall, F1-score, and AUC-ROC.\n","\n","Text classification is a versatile technique with wide-ranging applications, and the choice of approach depends on the specific requirements and constraints of the task at hand.\n"],"metadata":{"id":"fyouaQA9IkHC"}},{"cell_type":"markdown","source":["# 1. Text Classification Using Random Forest Classifier"],"metadata":{"id":"00N5QUBZIvEH"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","\n","# Load dataset\n","newsgroups = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n","X, y = newsgroups.data, newsgroups.target\n","\n","# Split dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Vectorize text data\n","vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n","X_train_vec = vectorizer.fit_transform(X_train)\n","X_test_vec = vectorizer.transform(X_test)\n","\n","# Train Random Forest Classifier\n","rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_classifier.fit(X_train_vec, y_train)\n","\n","# Predict and evaluate\n","y_pred = rf_classifier.predict(X_test_vec)\n","print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred, target_names=newsgroups.target_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2sRPKWmWIf9R","executionInfo":{"status":"ok","timestamp":1744128251400,"user_tz":240,"elapsed":73355,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"f99f2ee1-56e4-49ec-9d23-08130240bbc5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest Classification Report:\n","                           precision    recall  f1-score   support\n","\n","             alt.atheism       0.88      0.81      0.84       151\n","           comp.graphics       0.70      0.70      0.70       202\n"," comp.os.ms-windows.misc       0.73      0.87      0.80       195\n","comp.sys.ibm.pc.hardware       0.60      0.68      0.64       183\n","   comp.sys.mac.hardware       0.86      0.79      0.82       205\n","          comp.windows.x       0.89      0.73      0.80       215\n","            misc.forsale       0.78      0.78      0.78       193\n","               rec.autos       0.85      0.83      0.84       196\n","         rec.motorcycles       0.91      0.92      0.91       168\n","      rec.sport.baseball       0.83      0.90      0.86       211\n","        rec.sport.hockey       0.90      0.93      0.91       198\n","               sci.crypt       0.93      0.90      0.91       201\n","         sci.electronics       0.73      0.65      0.69       202\n","                 sci.med       0.81      0.90      0.85       194\n","               sci.space       0.85      0.93      0.89       189\n","  soc.religion.christian       0.81      0.96      0.88       202\n","      talk.politics.guns       0.80      0.88      0.84       188\n","   talk.politics.mideast       0.94      0.89      0.92       182\n","      talk.politics.misc       0.84      0.71      0.77       159\n","      talk.religion.misc       0.83      0.54      0.66       136\n","\n","                accuracy                           0.82      3770\n","               macro avg       0.82      0.81      0.82      3770\n","            weighted avg       0.82      0.82      0.82      3770\n","\n"]}]},{"cell_type":"markdown","source":["# 2. Text Classification Using LSTM"],"metadata":{"id":"yPE_FLROK0Rv"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n","from tensorflow.keras.utils import to_categorical\n","\n","# Parameters\n","max_words = 5000\n","max_len = 300\n","\n","# Tokenize and pad sequences\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(X_train)\n","X_train_seq = tokenizer.texts_to_sequences(X_train)\n","X_test_seq = tokenizer.texts_to_sequences(X_test)\n","X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n","X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n","\n","# Convert labels to categorical\n","y_train_cat = to_categorical(y_train, num_classes=20)\n","y_test_cat = to_categorical(y_test, num_classes=20)\n","\n","# Build LSTM model\n","model = Sequential()\n","model.add(Embedding(max_words, 100, input_length=max_len))\n","model.add(SpatialDropout1D(0.2))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(20, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train model\n","model.fit(X_train_pad, y_train_cat, epochs=5, batch_size=64, validation_data=(X_test_pad, y_test_cat))\n","\n","# Evaluate model\n","loss, accuracy = model.evaluate(X_test_pad, y_test_cat, verbose=0)\n","print(f\"LSTM Accuracy: {accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mi2Vzu5iI_bU","executionInfo":{"status":"ok","timestamp":1744129532719,"user_tz":240,"elapsed":1090302,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"607467a4-d695-462d-f54a-cf42ceb27fa4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 882ms/step - accuracy: 0.1097 - loss: 2.8823 - val_accuracy: 0.2828 - val_loss: 2.2313\n","Epoch 2/5\n","\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 811ms/step - accuracy: 0.3031 - loss: 2.1527 - val_accuracy: 0.3897 - val_loss: 1.8452\n","Epoch 3/5\n","\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 804ms/step - accuracy: 0.4364 - loss: 1.7473 - val_accuracy: 0.4708 - val_loss: 1.6094\n","Epoch 4/5\n","\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 761ms/step - accuracy: 0.5063 - loss: 1.5050 - val_accuracy: 0.5154 - val_loss: 1.4770\n","Epoch 5/5\n","\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 767ms/step - accuracy: 0.5715 - loss: 1.2868 - val_accuracy: 0.5279 - val_loss: 1.4990\n","LSTM Accuracy: 0.5278514623641968\n"]}]},{"cell_type":"markdown","source":["# 3. Text Classification Using Transfer Learning"],"metadata":{"id":"TAeEF-dLKsec"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","# Update the import for AdamW\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.optim import AdamW  # Import AdamW from torch.optim instead\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","import numpy as np\n","\n","data = {\n","    'text': [\n","        \"I love this product!\",\n","        \"This is the worst experience I've had.\",\n","        \"It's an okay service.\",\n","        \"Absolutely fantastic!\",\n","        \"Terrible, do not recommend.\"\n","    ],\n","    'label': [1, 0, 1, 1, 0]  # labels for positive (1) and negative (0) sentiment\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Split data into training and testing sets\n","train, test = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Load BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Create a custom dataset\n","class NewsDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=False,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation=True\n","        )\n","        input_ids = inputs['input_ids'].flatten()\n","        attention_mask = inputs['attention_mask'].flatten()\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n","        }\n","\n","# Create DataLoader\n","def create_data_loader(df, tokenizer, max_len, batch_size):\n","    ds = NewsDataset(\n","        texts=df['text'].to_numpy(),\n","        labels=df['label'].to_numpy(),\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        num_workers=2,\n","        shuffle=True  # Added shuffle=True for training data\n","    )\n","\n","# Set parameters\n","MAX_LEN = 128\n","BATCH_SIZE = 16\n","\n","# Create DataLoader for training and validation sets\n","train_data_loader = create_data_loader(train, tokenizer, MAX_LEN, BATCH_SIZE)\n","val_data_loader = create_data_loader(test, tokenizer, MAX_LEN, BATCH_SIZE)\n","\n","# Load BERT model\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Set up optimizer - fixed import and parameters\n","# AdamW from torch.optim doesn't have correct_bias parameter\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# Select device before model is moved to it\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Training loop\n","def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n","    model = model.train()\n","    losses = []\n","    correct_predictions = 0\n","\n","    for d in data_loader:\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"labels\"].to(device)\n","\n","        # Zero gradients first before forward pass\n","        optimizer.zero_grad()\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        loss = outputs.loss\n","        logits = outputs.logits\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Move predictions to CPU for comparison\n","        preds = torch.argmax(logits, dim=1)\n","        correct_predictions += torch.sum(preds == labels).item()\n","\n","    return correct_predictions / n_examples, np.mean(losses)\n","\n","# Evaluation loop\n","def eval_model(model, data_loader, device, n_examples):\n","    model = model.eval()\n","    losses = []\n","    correct_predictions = 0\n","\n","    with torch.no_grad():\n","        for d in data_loader:\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            labels = d[\"labels\"].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            loss = outputs.loss\n","            logits = outputs.logits\n","            losses.append(loss.item())\n","\n","            # Move predictions to CPU for comparison\n","            preds = torch.argmax(logits, dim=1)\n","            correct_predictions += torch.sum(preds == labels).item()\n","\n","    return correct_predictions / n_examples, np.mean(losses)\n","\n","# Define epochs\n","EPOCHS = 10\n","\n","# Create scheduler after optimizer is defined\n","from transformers import get_linear_schedule_with_warmup\n","\n","total_steps = len(train_data_loader) * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")\n","\n","# Main training loop\n","for epoch in range(EPOCHS):\n","    print(f'Epoch {epoch + 1}/{EPOCHS}')\n","\n","    train_acc, train_loss = train_epoch(\n","        model,\n","        train_data_loader,\n","        optimizer,\n","        device,\n","        scheduler,\n","        len(train)\n","    )\n","    print(f'Train loss {train_loss:.4f} accuracy {train_acc:.4f}')\n","\n","    val_acc, val_loss = eval_model(\n","        model,\n","        val_data_loader,\n","        device,\n","        len(test)\n","    )\n","    print(f'Val   loss {val_loss:.4f} accuracy {val_acc:.4f}\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_shYNwIxJ_-t","executionInfo":{"status":"ok","timestamp":1744130676543,"user_tz":240,"elapsed":119877,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"86b5e2f3-7099-410c-804d-5f7d713fcef6"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","Train loss 0.6210 accuracy 0.7500\n","Val   loss 0.8118 accuracy 0.0000\n","\n","Epoch 2/10\n","Train loss 0.5974 accuracy 0.7500\n","Val   loss 0.8570 accuracy 0.0000\n","\n","Epoch 3/10\n","Train loss 0.5589 accuracy 0.7500\n","Val   loss 0.9022 accuracy 0.0000\n","\n","Epoch 4/10\n","Train loss 0.4992 accuracy 0.7500\n","Val   loss 0.9414 accuracy 0.0000\n","\n","Epoch 5/10\n","Train loss 0.4452 accuracy 0.7500\n","Val   loss 0.9768 accuracy 0.0000\n","\n","Epoch 6/10\n","Train loss 0.4101 accuracy 1.0000\n","Val   loss 0.9987 accuracy 0.0000\n","\n","Epoch 7/10\n","Train loss 0.4320 accuracy 0.7500\n","Val   loss 1.0201 accuracy 0.0000\n","\n","Epoch 8/10\n","Train loss 0.4063 accuracy 0.7500\n","Val   loss 1.0363 accuracy 0.0000\n","\n","Epoch 9/10\n","Train loss 0.4214 accuracy 0.7500\n","Val   loss 1.0464 accuracy 0.0000\n","\n","Epoch 10/10\n","Train loss 0.4040 accuracy 1.0000\n","Val   loss 1.0518 accuracy 0.0000\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9kpBDkcDON9p"},"execution_count":null,"outputs":[]}]}